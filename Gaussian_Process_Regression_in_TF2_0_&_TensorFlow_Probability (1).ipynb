{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gaussian Process Regression in TF2.0 & TensorFlow Probability ",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 2",
      "name": "python2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ADKY4re5Kx-5"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S2AOrHzjK0_L",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\"); { display-mode: \"form\" }\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "56dF5DnkKx0a"
      },
      "source": [
        "# Gaussian Process Regression in TensorFlow Probability \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rTFa-AE9KxFC"
      },
      "source": [
        "In this tutorial, we explore Gaussian process regression using\n",
        "TensorFlow 2.0 and TensorFlow Probability. We generate some noisy observations from\n",
        "some known functions and fit GP models to those data. We will then sample from the GP\n",
        "posterior and plot the sampled function values over grids in their domains. This tutorial will also cover Markov Chain Monte Carlo methods and Deep Gaussian Processes (if time permits). \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ftkAb3cU5HNP"
      },
      "source": [
        "## Background\n",
        "\n",
        "###What is a Gaussian Process?###\n",
        "\n",
        "<img src=\"https://www.researchgate.net/profile/Miguel_Lazaro-Gredilla/publication/260637079/figure/fig1/AS:668972609458178@1536506907350/Example-of-a-Gaussian-process-posterior-in-12-with-20-training-samples-denoted-by.png\" width=\"300\">\n",
        "\n",
        "Gaussian processes are distributions over functions $f(x)$, which are defined by a mean function $m(x)$ and positive-definite covariance kernel $k(x, x^{'})$, with x being the function values and $(x, x^{'})$ being all the possible pairs of input data points. A function $f(x)$ of a Gaussian Process is therefore defined as:\n",
        "\n",
        "$$f(x) \\sim \\mathcal{GP}(m(x),k(x,x'))$$\n",
        "\n",
        "where for any finite subset $X = \\{x_{1} ... x_{n}\\}$ of the domain of $x$, the marginal distribution (probability distribution of the variables contained in the subset), is a multivariate Gaussian distribution, with a mean vector $\\mu = m(X)$ and covariance matrix $\\Sigma = k(X, X)$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QF_BW8m_c3_d"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQa005PGzNBS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow==2.0.0\n",
        "!pip install tfp-nightly\n",
        "!pip install gast==0.2.2\n",
        "!pip install quandl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "colab_type": "code",
        "id": "jw-_1yC50xaM",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "from tensorflow_probability import distributions as tfd\n",
        "from tensorflow_probability import positive_semidefinite_kernels as tfk\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "import scipy\n",
        "from time import time\n",
        "\n",
        "print(tf.__version__)\n",
        "print(tfp.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cu9S6c7uuvC1"
      },
      "source": [
        "## Example: Exact GP Regression on Noisy Sinusoidal Data\n",
        "Here we generate training data from a noisy sinusoid, then sample a bunch of\n",
        "curves from the posterior of the GP regression model. We use\n",
        "[Adam](https://arxiv.org/abs/1412.6980) to optimize the kernel hyperparameters\n",
        "(we minimize the negative log likelihood of the data under the prior). We\n",
        "plot the training curve, followed by the true function and the posterior\n",
        "samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Qrys68xzZE-c",
        "colab": {}
      },
      "source": [
        "def sinusoid(x):\n",
        "  return np.sin(3 * np.pi * x[..., 0])\n",
        "\n",
        "def generate_1d_data(num_training_points, observation_noise_variance):\n",
        "  \"\"\"Generate noisy sinusoidal observations at a random set of points.\n",
        "\n",
        "  Returns:\n",
        "     observation_index_points, observations\n",
        "  \"\"\"\n",
        "  index_points_ = np.random.uniform(-1., 1., (num_training_points, 1))\n",
        "  index_points_ = index_points_.astype(np.float64)\n",
        "\n",
        "  observations_ = (sinusoid(index_points_) +\n",
        "                   np.random.normal(loc=0,\n",
        "                                    scale=np.sqrt(observation_noise_variance),\n",
        "                                    size=(num_training_points)))\n",
        "  return index_points_, observations_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Tem9p8rUlqQR",
        "colab": {}
      },
      "source": [
        "# Generate training data with a known noise level (we'll later try to recover\n",
        "# this value from the data).\n",
        "NUM_TRAINING_POINTS = 100\n",
        "observation_index_points_, observations_ = generate_1d_data(\n",
        "    num_training_points=NUM_TRAINING_POINTS,\n",
        "    observation_noise_variance=.1)\n",
        "\n",
        "#Let's visualise our training data \n",
        "index_points_ = np.linspace(-1.2, 1.2, 200, dtype=np.float64)\n",
        "index_points_ = index_points_[..., np.newaxis]\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.title('Example Sinusoid Data')\n",
        "plt.plot(index_points_, sinusoid(index_points_),\n",
        "         label='True fn')\n",
        "plt.scatter(observation_index_points_[:, 0], observations_,\n",
        "            label='Observations')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3geePySJaSsM",
        "colab_type": "text"
      },
      "source": [
        "###Covariance function as a prior###\n",
        "\n",
        "\n",
        "<img src= \"http://inverseprobability.com/talks/slides/diagrams/kern/eq_covariance.gif\">\n",
        "\n",
        "The covariance function (a.k.a kernel function), which describes the covariance between each input pair, is what determines the distribution over a sample function $f(x)$. Therefore, we are effectively setting a prior information over our data by choosing a specific covariance function. \n",
        "\n",
        "In this tutorial, we make use of the\n",
        "ExponentiatedQuadratic covariance function (though you're more than welcome to try other covariance functions from the TFP library). Its form is\n",
        "\n",
        "$$\n",
        "k(x, x') := \\sigma^2 \\exp \\left( \\frac{\\|x - x'\\|^2}{\\lambda^2} \\right)\n",
        "$$\n",
        "\n",
        "where $\\sigma^2$ is the *overall variance* ($\\sigma$ is sometimes called the 'amplitude') and $\\lambda$ the *length scale*.\n",
        "We will now define these kernel parameters and optimise them via a maximum likelihood procedure.\n",
        "\n",
        "The hyperparameter *observation noise variance* is an extra hyperparameter to be learned by our Gaussian Process model [(McHutchon & Rasmussen 2011)](http://mlg.eng.cam.ac.uk/pub/pdf/MchRas11.pdf). It allows us to take the noise in the observation into account. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ByXndE3pkA4x",
        "colab": {}
      },
      "source": [
        "# Create the trainable model parameters, which we'll subsequently optimize.\n",
        "# Note that we constrain them to be strictly positive.\n",
        "amplitude_ = tf.Variable(initial_value=1., name='amplitude_', dtype=np.float64)\n",
        "length_scale_ = tf.Variable(initial_value=1., name='length_scale_', dtype=np.float64)\n",
        "observation_noise_variance_ = tf.Variable(initial_value=1e-6,\n",
        "                                         name='observation_noise_variance_',\n",
        "                                         dtype=np.float64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0WQ5IvoStQ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Illustrate covariance matrix and function\n",
        "\n",
        "#define the covariance function \n",
        "def exponentiated_quadratic(xa, xb):\n",
        "    #Exponentiated quadratic  with Ïƒ=1\n",
        "    # L2 distance (Squared Euclidian)\n",
        "    return np.exp(-0.5 * scipy.spatial.distance.cdist(xa, xb, 'sqeuclidean'))\n",
        "    \n",
        "# Show covariance matrix from exponentiated quadratic function\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "xlim = (-3, 3)\n",
        "X = np.expand_dims(np.linspace(*xlim,num=25), 1)\n",
        "sigma = exponentiated_quadratic(X, X)\n",
        "\n",
        "# Plot covariance matrix\n",
        "im = ax1.imshow(sigma, cmap=cm.YlGnBu)\n",
        "cbar = plt.colorbar(\n",
        "    im, ax=ax1, fraction=0.045, pad=0.05)\n",
        "cbar.ax.set_ylabel('$k(x,x)$', fontsize=10)\n",
        "ax1.set_title((\n",
        "    'Exponentiated quadratic \\n'\n",
        "    'example of covariance matrix'))\n",
        "ax1.set_xlabel('x', fontsize=13)\n",
        "ax1.set_ylabel('x', fontsize=13)\n",
        "ticks = list(range(-3, 4))\n",
        "ax1.set_xticks(np.linspace(0, len(X)-1, len(ticks)))\n",
        "ax1.set_yticks(np.linspace(0, len(X)-1, len(ticks)))\n",
        "ax1.set_xticklabels(ticks)\n",
        "ax1.set_yticklabels(ticks)\n",
        "ax1.grid(False)\n",
        "\n",
        "# Show covariance with X=0\n",
        "xlim = (-4, 4)\n",
        "X = np.expand_dims(np.linspace(*xlim, num=100), 1)\n",
        "zero = np.array([[0]])\n",
        "sigma0 = exponentiated_quadratic(X, zero)\n",
        "\n",
        "# Make the plots\n",
        "ax2.plot(X[:,0], sigma0[:,0], label='$k(x,0)$')\n",
        "ax2.set_xlabel('x', fontsize=13)\n",
        "ax2.set_ylabel('covariance', fontsize=13)\n",
        "ax2.set_title((\n",
        "    'Exponentiated quadratic  covariance\\n'\n",
        "    'between $x$ and $0$'))\n",
        "ax2.set_xlim(*xlim)\n",
        "ax2.legend(loc=1)\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzrd3tPhbIaY",
        "colab_type": "text"
      },
      "source": [
        "###Optimising the Hyperparameters###\n",
        "\n",
        "Let's define the marginal likelihood $p(y | X, \\theta)$ of the GP distribution. By maximizing it based on our observed data $(X, y)$, we can find the optimal hyperparameters $\\hat\\theta$ of our GP model. The optimal hyperparameter is defined as follows:\n",
        "\n",
        "$$\\hat\\theta = \\underset{\\theta}{\\arg\\max}(p(y | X, \\theta))$$\n",
        "\n",
        "The marginal likelihood of the GP is the likelihood of a multivariate Gaussian distribution which is given as:\n",
        "\n",
        "$$p(y | \\mu, \\Sigma) = \\frac{1}{\\sqrt{(2\\pi)^d|\\Sigma|}} \\exp(-\\frac{1}{2}(y-\\mu)^{\\top}\\Sigma^{-1}(y-\\mu)) $$\n",
        "\n",
        "In our case, $\\mu$ and $\\Sigma$ are defined by the mean function $m_{\\theta}(x)$ and the covariance (kernel) function $k_{\\theta}(x,x')$ of our GP model. Therefore, the likelihood could be re-written as:\n",
        "\n",
        "$$p(y | \\mu, \\theta) = \\frac{1}{\\sqrt{(2\\pi)^d|\\Sigma_{\\theta}|}} \\exp(-\\frac{1}{2}(y-\\mu_{\\theta})^{\\top}\\Sigma_{\\theta}^{-1}(y-\\mu_{\\theta})) $$\n",
        "\n",
        "where $d$ is the dimensionality of the marginal and $|\\Sigma_{\\theta}|$ is the determinant of the covariance kernel. Now let's get rid of the exponent on the right hand side by taking the log on both sides. \n",
        "\n",
        "$$\\log p(y | X, \\theta) = -\\frac{1}{2}(y - \\mu_{\\theta})^{\\top} \\Sigma_{\\theta}^{-1} (y - \\mu_{\\theta}) - \\frac{1}{2} \\log |\\Sigma_{\\theta}| - \\frac{d}{2}\\log 2\\pi$$\n",
        "\n",
        "The optimal hyperparameter $\\hat\\theta$ can therefore be found by minimizing the negative log marginal likelihood:\n",
        "\n",
        "$$\\hat\\theta = \\underset{\\theta}{\\arg\\max}(p(y | X, \\theta)) = \\underset{\\theta}{\\arg\\min}\\log p(y | X, \\theta)$$\n",
        "\n",
        "*Note that the function neg_log_likelihood is decorated with @tf.function. This allows us to compile our function into a high performance Tensorflow graph, giving us the benefit of faster execution while still using natural Python syntax.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WU4YxZQg1UM8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function #  <- faster execution \n",
        "def neg_log_likelihood():\n",
        "    amplitude = np.finfo(np.float64).tiny + tf.nn.softplus(amplitude_)\n",
        "    length_scale = np.finfo(np.float64).tiny + tf.nn.softplus(length_scale_)\n",
        "    observation_noise_variance = np.finfo(np.float64).tiny + tf.nn.softplus(observation_noise_variance_)\n",
        "\n",
        "    kernel = tfk.ExponentiatedQuadratic(amplitude, length_scale)\n",
        "\n",
        "    gp = tfd.GaussianProcess(\n",
        "        kernel=kernel,\n",
        "        index_points=observation_index_points_,\n",
        "        observation_noise_variance=observation_noise_variance\n",
        "    )\n",
        "\n",
        "    return -gp.log_prob(observations_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sjidgik8nv13",
        "colab_type": "text"
      },
      "source": [
        "The [Adam optimizer](https://arxiv.org/pdf/1412.6980.pdf) will be used to tune our hyperparameters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZsLbcrT1aG8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(lr=0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRIfTSoNSIRo",
        "colab_type": "text"
      },
      "source": [
        "One of the key benefits of TF2.0 over TF1.0 is the use of eager execution by default. Eager Execution means tf.Tensor will work seamlessly with NumPy arrays and standard Python debugging tools can be used for immediate error reporting and inspection of results. \n",
        "\n",
        "Training and/or gradient computation in eager mode is done using tf.GradientTape."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4swUVjI0DZY4",
        "colab": {}
      },
      "source": [
        "# Now we optimize the model parameters.\n",
        "num_iters = 1000\n",
        "\n",
        "#initialize negative log-likelihood as an array so that we can print out the training history for inspection later on \n",
        "nlls = np.zeros(num_iters, np.float64)\n",
        "\n",
        "start = time()\n",
        "for i in range(num_iters):\n",
        "    nlls[i] = neg_log_likelihood()\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss = neg_log_likelihood()\n",
        "    grads = tape.gradient(loss, [amplitude_, length_scale_, observation_noise_variance_])\n",
        "    optimizer.apply_gradients(zip(grads, [amplitude_, length_scale_, observation_noise_variance_]))\n",
        "\n",
        "print(\"Time: %0.2f\" % (time() - start))\n",
        "print('Trained parameters:'.format(amplitude_))\n",
        "print('amplitude: {}'.format(amplitude_))\n",
        "print('length_scale: {}'.format(length_scale_))\n",
        "print('observation_noise_variance: {}'.format(observation_noise_variance_))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QKS1ZvcEuHZs",
        "colab": {}
      },
      "source": [
        "# Plot the loss evolution\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(nlls)\n",
        "plt.title('Negative Log-Marginal Likelihood during training')\n",
        "plt.xlabel(\"Training iteration\")\n",
        "plt.ylabel(\"NLL\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMOqBJb_5i-n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#let's re-define the kernel using the trained parameters\n",
        "amplitude = np.finfo(np.float64).tiny + tf.nn.softplus(amplitude_)\n",
        "length_scale = np.finfo(np.float64).tiny + tf.nn.softplus(length_scale_)\n",
        "observation_noise_variance = np.finfo(np.float64).tiny + tf.nn.softplus(observation_noise_variance_)\n",
        "\n",
        "kernel = tfk.ExponentiatedQuadratic(amplitude, length_scale)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abQRef20ilsq",
        "colab_type": "text"
      },
      "source": [
        "Using the trained hyperparameters, let's sample from the posterior distribution $p(y| X, \\theta)$ conditioned on observations $X$ and hyperparameters $\\theta$. We will draw samples at points other than the training inputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1DOkwqQEsXVs",
        "colab": {}
      },
      "source": [
        "predictive_index_points_ = np.linspace(-1.2, 1.2, 200, dtype=np.float64)\n",
        "# Reshape to [200, 1] -- 1 is the dimensionality of the feature space.\n",
        "predictive_index_points_ = predictive_index_points_[..., np.newaxis]\n",
        "\n",
        "gprm = tfd.GaussianProcessRegressionModel(\n",
        "    kernel=kernel,  # Reuse the same kernel instance, with the same params\n",
        "    index_points=predictive_index_points_,\n",
        "    observation_index_points=observation_index_points_,\n",
        "    observations=observations_,\n",
        "    observation_noise_variance=observation_noise_variance,\n",
        "    predictive_noise_variance=0.)\n",
        "\n",
        "#compute the mean and standard deviation of the predictions from the trained model\n",
        "posterior_mean = gprm.mean()\n",
        "posterior_std = gprm.stddev()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1XgqrfsSub15",
        "colab": {}
      },
      "source": [
        "# Plot the true function, observations, and posterior samples.\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.plot(predictive_index_points_, sinusoid(predictive_index_points_),\n",
        "         label='True fn')\n",
        "plt.scatter(observation_index_points_[:, 0], observations_,\n",
        "            label='Observations')\n",
        "\n",
        "plt.plot(predictive_index_points_, posterior_mean, c='r', label='Posterior Mean $\\mu$')\n",
        "plt.fill_between(predictive_index_points_[:,0],  posterior_mean - 2*posterior_std, posterior_mean + 2*posterior_std, alpha=.5, label='$\\mu \\pm 2\\sigma$')\n",
        "\n",
        "leg = plt.legend(loc='upper right')\n",
        "for lh in leg.legendHandles: \n",
        "    lh.set_alpha(1)\n",
        "plt.xlabel(r\"Index points ($\\mathbb{R}^1$)\")\n",
        "plt.ylabel(\"Observation space\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aZe4H-7jy0hR"
      },
      "source": [
        "*Note: if you run the above code several times, sometimes it looks great and\n",
        "other times it looks terrible! The maximum likelihood training of the parameters\n",
        "is quite sensitive and sometimes converges to poor models. The best approach\n",
        "is to use MCMC to marginalize the model hyperparameters.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAKEHP3yEGh2",
        "colab_type": "text"
      },
      "source": [
        "## Marginalisation of model hyperparameters using Markov-Chain Monte Carlo\n",
        "\n",
        "Here we use Tensorflow Probability's MCMC functionality to marginalise GP hyperparameters (kernel parameters & observation noise variance). Markov-Chain Monte Carlo (MCMC) techniques are methods for sampling from probability distributions using Markov chains. \n",
        "\n",
        "### What is Markov-Chain Monte Carlo?  ###\n",
        "To first understand what Monte Carlo is, let's take a distribution $p(x)$ and compute its expectation $E[x]$. The expectation is given as:\n",
        "\n",
        "$$ E[x] = \\int_{-\\infty}^{\\infty} xp(x) dx $$ \n",
        "\n",
        "Monte Carlo method simply approximates this integral by drawning $n$ samples from $p(x)$ and then evaluating:\n",
        "\n",
        "$$ E[x] \\approx \\frac{1}{n}\\sum_{i = 1}^{\\infty} x_{i}$$\n",
        "\n",
        "Similarly, predictive distribution for our GP model is computed by:\n",
        "\n",
        "$$ p(y^{*}|x^{*}, x, y) = \\int p(y^{*}|x^{*}, x, y, \\theta) p(\\theta|y,x)d\\theta \\approx \\frac{1}{N}\\sum_{n = 1}^{N} p(y^{*}|x^{*}, x, y, \\theta^{n}), \\quad\\theta^{n} \\sim p(\\theta|x,y)$$\n",
        "\n",
        "The issue here is that the integral is analytically intractable to compute as we cannot easily sample from $p(\\theta|x,y)$. To sample $p(\\theta|x,y)$, we need something else and that's where the Markov chain comes in. \n",
        "\n",
        "Consider a sequence of random variables $\\{x_{0}, x_{1} ..., x_{n}\\}$ sampled from a probability distribution $p(x_{t+1} | x_{t})$. A Markov chain is a sequence in which the probability of each next sample $x_{t+1}$ only depends on the current state $x_{t}$ and not on the further history $\\{x_{0}, x_{1}, .., x_{t-1}\\}$. \n",
        "\n",
        "Thus, the MCMC techniques attempts to approximate a target distribution by constructing cleverly sampled chains that draw samples, which are progressively more likely realisations of the distribution of interest; in the context of GP, that would be our hyperparameter posterior distribution $p(\\theta|y,x)$. \n",
        "\n",
        "Let's first define the log posterior distribution $\\log p(\\theta|y,x)$ and initialise our Markov chain."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_siF5S29Lzy7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def joint_log_prob(\n",
        "    index_points, observations, amplitude, length_scale, noise_variance):\n",
        "\n",
        "  # Hyperparameter Distributions.\n",
        "  rv_amplitude = tfd.LogNormal(np.float64(0.), np.float64(1))\n",
        "  rv_length_scale = tfd.LogNormal(np.float64(0.), np.float64(1))\n",
        "  rv_noise_variance = tfd.LogNormal(np.float64(0.), np.float64(1))\n",
        "\n",
        "  gp = tfd.GaussianProcess(\n",
        "      kernel=tfk.ExponentiatedQuadratic(amplitude, length_scale),\n",
        "      index_points=index_points,\n",
        "      observation_noise_variance=noise_variance)\n",
        "\n",
        "  return (\n",
        "      rv_amplitude.log_prob(amplitude) +\n",
        "      rv_length_scale.log_prob(length_scale) +\n",
        "      rv_noise_variance.log_prob(noise_variance) +\n",
        "      gp.log_prob(observations)\n",
        "  )\n",
        "\n",
        "# Construct a Markov chain of hyperparameter variables that we want to optimise\n",
        "initial_chain_states = [\n",
        "    1e-1 * tf.ones([], dtype=np.float64, name='init_amplitude'),\n",
        "    1e-1 * tf.ones([], dtype=np.float64, name='init_length_scale'),\n",
        "    1e-1 * tf.ones([], dtype=np.float64, name='init_obs_noise_variance')\n",
        "]\n",
        "\n",
        "#log_posterior distribution of the hyperparameters \n",
        "def unnormalized_log_posterior(amplitude, length_scale, noise_variance):\n",
        "  return joint_log_prob(\n",
        "      observation_index_points_, observations_, amplitude, length_scale,\n",
        "      noise_variance)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4puBKyPX4tqa",
        "colab_type": "text"
      },
      "source": [
        "*Note: Inference in MCMC is usually performed while ignoring the normalizing constant $p(y)$. Therefore, $p(\\theta| y) \\propto p(y|\\theta)p(\\theta)$ and the log hyperparameter posterior distribution can be expressed as $\\log p(y|\\theta) + \\log p(\\theta)$. *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bUjAkls6nUw",
        "colab_type": "text"
      },
      "source": [
        "###Metropolis-Hastings Algorithm###\n",
        "\n",
        "Now that we've initialised our Markov chain and defined our posterior hyperparameter distribution, we will use the **Metropolis-Hastings algorithm** to converge to our target distribution. \n",
        "\n",
        "The following things are to be specified for the Markov chain:\n",
        "\n",
        "*   probability distribution for the initial state $p(\\theta^{1})$\n",
        "*   Conditional probability for subsequent states in the form of transition probabilities $T(\\theta^{t+1} \\leftarrow \\theta^{t})$\n",
        "\n",
        "Note that $T(\\theta^{t+1} \\leftarrow \\theta^{t})$ is also known as the *transition kernel*. \n",
        "\n",
        "In the case of Metropolis-Hastings algorithm, the Markov chain transition operator is defined as follows:\n",
        "\n",
        "*   A new candidate state $\\theta^{*}$ proposed by a proposal distribution $q(\\theta^{*}|\\theta)$\n",
        "*   The probability of accepting the candidate state is $\\min(1, r)$, where $r = \\frac{p(\\theta^{*}|y)q(\\theta^{*}|\\theta^{t})}{p(\\theta^{t}|y)q(\\theta^{t}|\\theta^{*})}$ \n",
        "\n",
        "If the proposal is accepted, set $\\theta^{t+1} = \\theta^{*}$, otherwise $\\theta^{t+1} = \\theta^{t}$. \n",
        "\n",
        "###Hamiltonian Monte Carlo###\n",
        "\n",
        "**HMC (Hamiltonian Monte Carlo)** is a special case of Metropolis-Hastings algorithm that uses Hamiltonian dynamics to create proposal distribution for the Metropolis-Hastings algorithm that allows the state space to be explored more efficiently. The mathematical details of the algorithm is beyond the scope of this tutorial so we will only cover a brief summary of the algorithm, which is as follows:\n",
        "\n",
        "For a given number of iterations,\n",
        "\n",
        "1.   Sample a new momentum value $\\rho$ from an auxillary distribution.\n",
        "2.   Evolve the joint system $(\\theta, \\rho)$ via Hamilton's equations.\n",
        "3. Update $\\theta$ using the leapfrog integrator with discretisation time $\\epsilon$ and leapfrog steps L according to the Hamiltonian dynamics.\n",
        "4. Apply the Metropolis acceptance step to decide whether to update to a new state $(\\theta^{*}, \\rho^{*})$ or keep the existing state. \n",
        "\n",
        "For mathematical details, please refer to [Betancourt & Girolami 2013](https://arxiv.org/pdf/1312.0906.pdf) and [Neal 2012](https://arxiv.org/pdf/1206.1901.pdf). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bqc0IKdz1uLk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Transport map accelerated Markov chain Monte Carlo: https://arxiv.org/pdf/1412.5492.pdf##\n",
        "# Since HMC operates over unconstrained space, we need to transform the\n",
        "# samples so they live in real-space.\n",
        "unconstraining_bijectors = [\n",
        "    tfp.bijectors.Softplus(),\n",
        "    tfp.bijectors.Softplus(),\n",
        "    tfp.bijectors.Softplus(),\n",
        "]\n",
        "\n",
        "num_results = 200\n",
        "\n",
        "@tf.function   # <- make things fast\n",
        "def run_mcmc():\n",
        "  return tfp.mcmc.sample_chain(\n",
        "    num_results=num_results,\n",
        "    num_burnin_steps=500,\n",
        "    num_steps_between_results=3,\n",
        "    current_state=initial_chain_states,\n",
        "    kernel=tfp.mcmc.TransformedTransitionKernel(\n",
        "        inner_kernel = tfp.mcmc.HamiltonianMonteCarlo(\n",
        "            target_log_prob_fn=unnormalized_log_posterior,\n",
        "            step_size=[np.float64(.15)],\n",
        "            num_leapfrog_steps=3),\n",
        "        bijector=unconstraining_bijectors))\n",
        "  \n",
        "[amplitudes,length_scales,observation_noise_variances], kernel_results = run_mcmc()\n",
        "\n",
        "print(\"Acceptance rate: {}\".format(\n",
        "    np.mean(kernel_results.inner_results.is_accepted)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axTUN4jLS7re",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gprm = tfd.GaussianProcessRegressionModel(\n",
        "    kernel=tfk.ExponentiatedQuadratic(amplitudes, length_scales),\n",
        "    index_points=predictive_index_points_,\n",
        "    observation_index_points=observation_index_points_,\n",
        "    observations=observations_,\n",
        "    observation_noise_variance=observation_noise_variances,\n",
        "    predictive_noise_variance=0.)\n",
        "\n",
        "samples = np.transpose(gprm.sample())\n",
        "\n",
        "posterior_mean = np.mean(samples, axis=1)\n",
        "posterior_std = np.std(samples, axis=1)\n",
        "\n",
        "# Plot the true function, observations, and posterior samples.\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.plot(predictive_index_points_, sinusoid(predictive_index_points_),\n",
        "         label='True fn')\n",
        "plt.scatter(observation_index_points_[:, 0], observations_,\n",
        "            label='Observations')\n",
        "\n",
        "plt.plot(predictive_index_points_, posterior_mean, c='r', label='Posterior Prediction')\n",
        "plt.fill_between(predictive_index_points_[:,0],  posterior_mean - 2*posterior_std, posterior_mean + 2*posterior_std, alpha=.5)\n",
        "\n",
        "leg = plt.legend(loc='upper right')\n",
        "for lh in leg.legendHandles: \n",
        "    lh.set_alpha(1)\n",
        "plt.xlabel(r\"Index points ($\\mathbb{R}^1$)\")\n",
        "plt.ylabel(\"Observation space\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}